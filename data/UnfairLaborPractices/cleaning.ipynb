{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4717f24c-79cf-4c82-8a6a-9a14da1682bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ccf346-3bff-4c59-adef-ae54a444f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to import (avoids attempting to import empty columns)\n",
    "columns_to_import = [\"Case Type\",\"Region\",\"Case Number\",\"Case Name\",\"Status\",\"Date Filed\",\"Date Closed\",\"Reason Closed\"\n",
    "                     ,\"City\",\"States & Territories\",\"Employees on charge/petition\",\"Allegations\",\"Participants\",\"Union\",\n",
    "                     \"Unit Sought\",\"Voters\"]\n",
    "\n",
    "# Column data types, to avoid Pandas trying to parse data type on every column\n",
    "column_dtypes = {c:\"string\" for c in columns_to_import}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96c827f6-8979-43e5-b910-5012ffd82ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import various partial data files, parsing dates in the date columns. All other columns are strings.\n",
    "df1978_1999 = pd.read_csv(\"1978-1999.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "\n",
    "# Note: broken entries in rows 10108 - 10111\n",
    "df2000_2004 = pd.read_csv(\"2000-2004.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2005_2009 = pd.read_csv(\"2005-2009.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2010 = pd.read_csv(\"2010.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2011 = pd.read_csv(\"2011.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2012 = pd.read_csv(\"2012.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2013 = pd.read_csv(\"2013.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2014 = pd.read_csv(\"2014.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "\n",
    "# Note: broken entry at row 1196 and 1197\n",
    "df2015_2019 = pd.read_csv(\"2015-2019.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2020_2024 = pd.read_csv(\"2020-2024.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fac5ea1c-27b9-4299-9ecb-b0150a52e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As imported, data files are descending in date. Concatenate into one large data file.\n",
    "# According to NLRB website, we should have 412094 cases, but we get 413889 rows. The overcount\n",
    "# is probably due to rows that broke while downloading. TODO: check which years need to be redone.\n",
    "unfair_labor_practices = pd.concat([df2020_2024, df2015_2019, df2014, df2013, df2012, df2011, df2010, df2005_2009, df2000_2004, df1978_1999], ignore_index = True)\n",
    "\n",
    "# When imported, missing values became \"<NA>\". Replace these with NaT to match usual Pandas convention by \n",
    "# replacing \"<NA>\" by pd.NaT, then using to_datetime and coercing NaTs.\n",
    "unfair_labor_practices[\"Date Closed\"] = pd.to_datetime(unfair_labor_practices[\"Date Closed\"].replace(\"<NA>\", pd.NaT), errors = \"coerce\")\n",
    "unfair_labor_practices[\"Date Filed\"] = pd.to_datetime(unfair_labor_practices[\"Date Filed\"].replace(\"<NA>\", pd.NaT), errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "318c0702-000e-4cee-ac16-a1261f309473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000: 3919, 2001: 5164, 2002: 7595, 2003: 9135, 2004: 7252, 2005: 8911, 2006: 13048, 2007: 22987, 2008: 22190, 2009: 24849, 2010: 23671, 2011: 21469, 2012: 21216, 2013: 21334, 2014: 27402, 2015: 20044, 2016: 21032, 2017: 19146, 2018: 18891, 2019: 18343, 2020: 15139, 2021: 15534, 2022: 18609, 2023: 20305, 2024: 6099}\n"
     ]
    }
   ],
   "source": [
    "# Print case counts by year.\n",
    "print({year: unfair_labor_practices[unfair_labor_practices[\"Date Filed\"].dt.year == year].shape[0] for year in range(2000,2025)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc6a50c0-33d4-442b-8317-f455ffdfcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The counts that match NLRB website are in years\n",
    "# pre2000, 2000, 2001, 2002, 2005, 2006, 2007, 2008, 2010, 2011, 2012, 2015, 2016, 2017, 2018, 2021\n",
    "# Get rid of all other years---we'll add them back via fixed files below.\n",
    "years_filed =  unfair_labor_practices[\"Date Filed\"].dt.year\n",
    "filt = years_filed.isin([2000, 2001, 2002, 2005, 2006, 2007, 2008, 2010, 2011, 2012, 2015, 2016, 2017, 2018, 2021]) | (years_filed < 2000)\n",
    "unfair_labor_practices = unfair_labor_practices[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c572613c-d5f3-4a20-b85c-7d4e57472555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process manually downloaded CSVs for years that didn't match NLRB website.\n",
    "# These CSVs also don't quite match the NLRB website count but at least are devoid\n",
    "# of broken entries.\n",
    "df2003 = pd.read_csv(\"2003.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2004 = pd.read_csv(\"2004.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2009 = pd.read_csv(\"2009.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2013 = pd.read_csv(\"2013.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2014 = pd.read_csv(\"2014.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2019 = pd.read_csv(\"2019.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2020 = pd.read_csv(\"2020.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2022 = pd.read_csv(\"2022.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2023 = pd.read_csv(\"2023.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")\n",
    "df2024 = pd.read_csv(\"2024.csv\", usecols = columns_to_import, dtype = column_dtypes, parse_dates = [\"Date Filed\", \"Date Closed\"], date_format = \"%m%d%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cd6b22ce-79b7-4f11-8d07-72dbb5b227e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432137"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfair_labor_practices_patch = pd.concat([df2003,df2004,df2009,df2013,df2014,df2019,df2020,df2022,df2023,df2024], ignore_index = True)\n",
    "\n",
    "# When imported, missing values became \"<NA>\". Replace these with NaT to match usual Pandas convention by \n",
    "# replacing \"<NA>\" by pd.NaT, then using to_datetime and coercing NaTs.\n",
    "unfair_labor_practices_patch[\"Date Closed\"] = pd.to_datetime(unfair_labor_practices_patch[\"Date Closed\"].replace(\"<NA>\", pd.NaT), errors = \"coerce\")\n",
    "unfair_labor_practices_patch[\"Date Filed\"] = pd.to_datetime(unfair_labor_practices_patch[\"Date Filed\"].replace(\"<NA>\", pd.NaT), errors = \"coerce\")\n",
    "\n",
    "unfair_labor_practices = pd.concat([unfair_labor_practices, unfair_labor_practices_patch])\n",
    "unfair_labor_practices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ccb17733-2b6d-475d-9303-ce07a7e9fa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428525"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove entries where case type is not \"C\" (removes accidentally included representation petitions,\n",
    "# which are the vast majority of removed entries, plus some broken entries)\n",
    "unfair_labor_practices = unfair_labor_practices[unfair_labor_practices[\"Case Type\"] == \"C\"]\n",
    "unfair_labor_practices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c057eab8-f1ec-45e5-9ab7-4413bae3c837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428503"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove broken entries by checking case number formatting\n",
    "unfair_labor_practices = unfair_labor_practices[unfair_labor_practices[\"Case Number\"].str.match(r'..-..-*')]\n",
    "unfair_labor_practices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b11c71f9-b6ca-4618-a1f5-2f87ffe60348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428068"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove broken entries by checking that \"Union\", \"Unit Sought\", and \"Voters\" columns\n",
    "# are empty. Some of these entries include case numbers that we could go through\n",
    "# and download fixed copies of the broken cases. However there are only 435 rows\n",
    "# that this removes, so it's probably not a huge deal.\n",
    "filt = unfair_labor_practices[\"Union\"].isna() & unfair_labor_practices[\"Unit Sought\"].isna() & unfair_labor_practices[\"Voters\"].isna()\n",
    "unfair_labor_practices = unfair_labor_practices[filt]\n",
    "unfair_labor_practices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d835d0f-7a46-4ab1-9239-3e070eaa0052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400340"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we drop duplicate rows\n",
    "unfair_labor_practices = unfair_labor_practices.drop_duplicates()\n",
    "unfair_labor_practices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f5696e31-05ef-4b07-9be4-d59bfd5dd827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to year-by-year files (pre2000 grouped in their own file)\n",
    "\n",
    "columns_to_export = [\"Case Type\",\"Region\",\"Case Number\",\"Case Name\",\"Status\",\"Date Filed\",\"Date Closed\",\"Reason Closed\"\n",
    "                     ,\"City\",\"States & Territories\",\"Employees on charge/petition\",\"Allegations\",\"Participants\"]\n",
    "\n",
    "filt = (unfair_labor_practices[\"Date Filed\"].dt.year < 2000)\n",
    "unfair_labor_practices[filt].drop_duplicates().to_csv(\"cleaned/pre2000.csv\", columns = columns_to_export )\n",
    "for year in range(2000,2025):\n",
    "    filt = (unfair_labor_practices[\"Date Filed\"].dt.year == year)\n",
    "    unfair_labor_practices[filt].drop_duplicates().to_csv(\"cleaned/\"+str(year)+\".csv\", columns = columns_to_export )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11407c39-9199-41a7-acf0-806db8e49f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
